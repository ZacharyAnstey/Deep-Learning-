{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c809bc6e",
   "metadata": {},
   "source": [
    "# Build The Neural Network\n",
    "Neural networks comprise of layers that perfrom operations on data. The torch.nn namespace provides all the building blocks you need to build your own neural network. A neural network is a module itself that consists of other modules. The nested structure allows for building and mangaing complex architecutres easily \n",
    "<p> In the following sections, we will build a neural network to classify images in the FashionMNIST dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbda4dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524cb616",
   "metadata": {},
   "source": [
    "## Get Device for training \n",
    "We want to be able to train our model on a GPU. To do this we have to check that torch.cuda is available, else we continue on the CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cc267a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8039d73",
   "metadata": {},
   "source": [
    "## Define the Class \n",
    "We define our neural network by subclassing nn.module, and initalizing the neural network layers init. Every nn.Module subclass implements the operations on input data in the forward method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "061e7e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork,self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "        nn.Linear(28*28,512),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(512,512),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(512,10),\n",
    "        nn.ReLU()\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e56a4d",
   "metadata": {},
   "source": [
    "We create an instance of NeuralNetwork, and move it to the decive and print its structure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed14f0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d160e6f1",
   "metadata": {},
   "source": [
    "To use the model, we pass it the input data. This executes the models forward along with some backpropagation operations. Caling the model on the input returns a 10 dimensional tensor with raw predicted values for each class. We get the production probabilites by passing it thourgh an instance of the nn.Softmax module \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54ac385a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted calss: tensor([3], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1,28,28,device=device)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted calss: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342ba4f1",
   "metadata": {},
   "source": [
    "## Model Layers \n",
    "Lets break down the layers in the FashionMNIST model. To illustrate it, we will take a sample minibatch of 3 images of size 28x28 and see what happens to it as we pass through the network \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bbc5d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.rand(3,28,28)\n",
    "print(input_image.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2c4e5f",
   "metadata": {},
   "source": [
    "## nn.Flatten \n",
    "We initalize the nn.Flatten layer to convert each 2D 28 x 28 image into a contiguous array of 784 pixel values \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d924b509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90fe55d",
   "metadata": {},
   "source": [
    "## nn.Linear \n",
    "The linear layer is a module that applies a linear transfomration on the input using its stored weights and biases \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "492bf43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dd85be",
   "metadata": {},
   "source": [
    "## nn.ReLU\n",
    "Non-linear activations are what create the complex mappings between the models inputs and outputs. They are applied after linear transformations to introduce nonlinearity, helping neural networks learn a wide variety of phenomena. In this model we use nn.ReLU between our linear layers, but there is other activations to introduce non linearity in your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7276e948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[-0.4856, -0.1061, -0.1030,  0.6917, -0.3450,  0.4654,  0.0239,  0.0550,\n",
      "         -0.2685,  0.5198, -0.1533,  0.4484,  0.3980,  0.0649,  0.2138,  0.3012,\n",
      "         -0.0321,  0.4431, -0.2107, -0.3729],\n",
      "        [-0.2920, -0.0472, -0.3578,  0.1594, -0.2229,  0.1289, -0.0276, -0.1596,\n",
      "         -0.1361,  0.2998, -0.0123, -0.1129,  0.1103,  0.1302, -0.0641,  0.3832,\n",
      "          0.1046,  0.1503,  0.0341, -0.0296],\n",
      "        [-0.0840, -0.2090, -0.2601,  0.1261, -0.5185, -0.1118, -0.0988,  0.2570,\n",
      "         -0.2266,  0.4744, -0.1395,  0.1325,  0.3224,  0.4505,  0.1592,  0.0713,\n",
      "         -0.0541,  0.3423, -0.4992, -0.2150]], grad_fn=<AddmmBackward>) \n",
      "\n",
      "\n",
      "After ReLU:tensor([[0.0000, 0.0000, 0.0000, 0.6917, 0.0000, 0.4654, 0.0239, 0.0550, 0.0000,\n",
      "         0.5198, 0.0000, 0.4484, 0.3980, 0.0649, 0.2138, 0.3012, 0.0000, 0.4431,\n",
      "         0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1594, 0.0000, 0.1289, 0.0000, 0.0000, 0.0000,\n",
      "         0.2998, 0.0000, 0.0000, 0.1103, 0.1302, 0.0000, 0.3832, 0.1046, 0.1503,\n",
      "         0.0341, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1261, 0.0000, 0.0000, 0.0000, 0.2570, 0.0000,\n",
      "         0.4744, 0.0000, 0.1325, 0.3224, 0.4505, 0.1592, 0.0713, 0.0000, 0.3423,\n",
      "         0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before ReLU: {hidden1} \\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU:{hidden1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad5bfcc",
   "metadata": {},
   "source": [
    "## nn.Sequential \n",
    "n.Sequential is an ordered container of modules. The data is passes through all the modules in the same rder as defined. You can suquential containers to put together a quick network like seq_modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8011710a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_modules = nn.Sequential(\n",
    "flatten,\n",
    "layer1,\n",
    "nn.Linear(20,10)\n",
    ")\n",
    "input_image = torch.rand(3,28,28)\n",
    "logits = seq_modules(input_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1665a6",
   "metadata": {},
   "source": [
    "## nn.Softmax \n",
    "The last layer of the neural network returns logits - raw values in [-infinity, infitiy] which are passed on the nn.Softmax module. The logits are scaled to values [0,1] representing the model's predictied probabilities for each class. dim parameter indicated the fimension along which the values must sum to 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ca8e19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probab = softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e676dc",
   "metadata": {},
   "source": [
    "## Model Parmeters \n",
    "Many layers inside a neural network are parameterized, i.e have associated weights and biases that are optimized during trainging Subclassing nn.Module automatically tracks defined inside your model object, and makes all parameters accessible using your models parameters() or named_parameters() methods \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69f5f1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module Structure: NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ") \n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: <built-in method size of Parameter object at 0x000001FAFC2A8400> | Values: tensor([[-0.0191, -0.0246, -0.0105,  ...,  0.0002, -0.0156,  0.0342],\n",
      "        [ 0.0015,  0.0018, -0.0253,  ..., -0.0254, -0.0276, -0.0092]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: <built-in method size of Parameter object at 0x000001FAFC2A81C0> | Values: tensor([ 0.0097, -0.0169], device='cuda:0', grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: <built-in method size of Parameter object at 0x000001FAD74A2C80> | Values: tensor([[-0.0438,  0.0257,  0.0334,  ..., -0.0147,  0.0124, -0.0226],\n",
      "        [-0.0021, -0.0385, -0.0429,  ..., -0.0212,  0.0428,  0.0085]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: <built-in method size of Parameter object at 0x000001FAFC2A8540> | Values: tensor([-0.0180,  0.0007], device='cuda:0', grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: <built-in method size of Parameter object at 0x000001FAFC2A8580> | Values: tensor([[-0.0089,  0.0018, -0.0037,  ...,  0.0262,  0.0015, -0.0157],\n",
      "        [-0.0103, -0.0153, -0.0032,  ..., -0.0384,  0.0429,  0.0391]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: <built-in method size of Parameter object at 0x000001FAFC2A8500> | Values: tensor([-0.0288, -0.0158], device='cuda:0', grad_fn=<SliceBackward>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Module Structure:',model, '\\n\\n')\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size} | Values: {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b29f5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
