{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1808661f",
   "metadata": {},
   "source": [
    "# Pytorch tutorial 1 \n",
    "this is a tutorial on how to use pytorch building a basic neural network "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687fb666",
   "metadata": {},
   "source": [
    "First we have to import the libaries that are needed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3f23e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c10657c",
   "metadata": {},
   "source": [
    "Now that we have the necessary libaries imported we can now  import out test and training data from the MNIST data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86d1e3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ceba095",
   "metadata": {},
   "source": [
    "Now that we have the test and training data loaded we can now pass the dataset to dataloader. This will wrap an iterable over our dataset, and supporst batching, sampling, shuffling and multiprocess data loading. We are going to define a batch size of 64. This means that each element in the dataloader iterable will return a batch of 64 features and labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9784d709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]:  torch.Size([64, 1, 28, 28])\n",
      "Shape of y:  torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(\"Shape of X [N, C, H, W]: \", X.shape)\n",
    "    print(\"Shape of y: \", y.shape, y.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dae41c",
   "metadata": {},
   "source": [
    "Now we can define a nural network, we do this by using nn.Module. We are going to define two functions a init function and a forward function.  We define the layers of the network in the __ in it __ function. And we specify how data will pass through the network in the forward function. The speed things up we will use a GPU if available if not we will use CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68569dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcd9e88",
   "metadata": {},
   "source": [
    "We know that to train a neural network we need two things \n",
    "- 1) loss fucntion\n",
    "- 2) optimizer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee2b9d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db062771",
   "metadata": {},
   "source": [
    " Now we are going to define a function for training our model and backpropagating the error to adjust the hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44243673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132b6c7f",
   "metadata": {},
   "source": [
    "We now are going to check the models perfromacne againgst the test dataset. This will tell us that the model is in fact learning. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5cad619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26f706f",
   "metadata": {},
   "source": [
    "Training is conducted over multiple iterations called epochs. During each epoch, the model learns paraemters to make better predictions. We will print the models accuracy and loss at each epoch. We should see the accuracy incrase and the loss decrease through each epoch     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba9098d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.306691  [    0/60000]\n",
      "loss: 2.301905  [ 6400/60000]\n",
      "loss: 2.297332  [12800/60000]\n",
      "loss: 2.287883  [19200/60000]\n",
      "loss: 2.276832  [25600/60000]\n",
      "loss: 2.274376  [32000/60000]\n",
      "loss: 2.268922  [38400/60000]\n",
      "loss: 2.270203  [44800/60000]\n",
      "loss: 2.251479  [51200/60000]\n",
      "loss: 2.225602  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.6%, Avg loss: 0.035184 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.254609  [    0/60000]\n",
      "loss: 2.259766  [ 6400/60000]\n",
      "loss: 2.246105  [12800/60000]\n",
      "loss: 2.223297  [19200/60000]\n",
      "loss: 2.207486  [25600/60000]\n",
      "loss: 2.198854  [32000/60000]\n",
      "loss: 2.183946  [38400/60000]\n",
      "loss: 2.182044  [44800/60000]\n",
      "loss: 2.152265  [51200/60000]\n",
      "loss: 2.111481  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 44.6%, Avg loss: 0.033513 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.168022  [    0/60000]\n",
      "loss: 2.174188  [ 6400/60000]\n",
      "loss: 2.131421  [12800/60000]\n",
      "loss: 2.096326  [19200/60000]\n",
      "loss: 2.094464  [25600/60000]\n",
      "loss: 2.058376  [32000/60000]\n",
      "loss: 2.037386  [38400/60000]\n",
      "loss: 2.023818  [44800/60000]\n",
      "loss: 1.964959  [51200/60000]\n",
      "loss: 1.929463  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 47.0%, Avg loss: 0.030722 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.013469  [    0/60000]\n",
      "loss: 2.024196  [ 6400/60000]\n",
      "loss: 1.946008  [12800/60000]\n",
      "loss: 1.897206  [19200/60000]\n",
      "loss: 1.928737  [25600/60000]\n",
      "loss: 1.870319  [32000/60000]\n",
      "loss: 1.837082  [38400/60000]\n",
      "loss: 1.820822  [44800/60000]\n",
      "loss: 1.731608  [51200/60000]\n",
      "loss: 1.726663  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 0.027573 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.834521  [    0/60000]\n",
      "loss: 1.855741  [ 6400/60000]\n",
      "loss: 1.749437  [12800/60000]\n",
      "loss: 1.705666  [19200/60000]\n",
      "loss: 1.777382  [25600/60000]\n",
      "loss: 1.699659  [32000/60000]\n",
      "loss: 1.666276  [38400/60000]\n",
      "loss: 1.658095  [44800/60000]\n",
      "loss: 1.550402  [51200/60000]\n",
      "loss: 1.583856  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 54.4%, Avg loss: 0.025221 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48042599",
   "metadata": {},
   "source": [
    "Now we are going to save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eadcb53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Pytorch model state to model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(),'model.pth')\n",
    "print('Saved Pytorch model state to model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0517ba2e",
   "metadata": {},
   "source": [
    " The process for loading a model includes re creating the model structure and loading the data dictionary into it \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e10caf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork()\n",
    "model.load_state_dict(torch.load('model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c97c50fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"Sneaker\",Actual: \"Ankle boot\"\n"
     ]
    }
   ],
   "source": [
    "classes = [\n",
    "    'T-shirt/top',\n",
    "    'Trouser',\n",
    "    'Pullover',\n",
    "    'Dress',\n",
    "    'Coat',\n",
    "    'Sandal',\n",
    "    'Shirt',\n",
    "    'Sneaker',\n",
    "    'Bag',\n",
    "    'Ankle boot',\n",
    "    \n",
    "]\n",
    "model.eval()\n",
    "x,y=test_data[0][0],test_data[0][1]\n",
    "with torch.no_grad():\n",
    "    pred = model(x)\n",
    "    predicted,actual = classes[pred[0].argmax(0)],classes[y]\n",
    "    print(f'Predicted: \"{predicted}\",Actual: \"{actual}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dbc042",
   "metadata": {},
   "source": [
    "# References "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66df485e",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1170a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
